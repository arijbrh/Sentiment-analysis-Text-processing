{"cells":[{"cell_type":"markdown","metadata":{"id":"3jDtHv3z-TOi"},"source":["# Text Processing and Word Embeddings\n","\n","Welcome to this new exercise! In this exercise, we will play around with text instead of images as before, using Recurrent Neural Networks. Generally, it is called Natural Language Processing (NLP) when dealing with text, speech, etc. But the data structure is very different from images, i.e., text is a string, while images consist of numbers. Hence, we need some preprocessing steps to transform the raw text into another data format. This notebook will introduce these basic concepts in NLP pipelines. Specifically, you will learn about:\n","\n","1. How to preprocess text classification datasets\n","2. How to create a simple word embedding layer that maps words to dense vectors"]},{"cell_type":"markdown","metadata":{"id":"QcigxPLp-TOl"},"source":["## (Optional) Mount folder in Colab\n","\n","Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_O5dk2Ds-TOl","executionInfo":{"status":"ok","timestamp":1688928402840,"user_tz":-120,"elapsed":17513,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"a2450c58-54fe-40da-e42d-ae08614f8b8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['1_text_preprocessing_and_embedding.ipynb', '2_sentiment_analysis.ipynb', 'Optional-recurrent_neural_networks.ipynb', 'exercise_code', 'images']\n"]}],"source":["# Use the following lines if you want to use Google Colab\n","# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n","# NOTE: terminate all other colab sessions that use GPU!\n","# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n","\n","\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n"]},{"cell_type":"markdown","metadata":{"id":"kbStF2L1-TOn"},"source":["### Set up PyTorch environment in colab\n","- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n","- Uncomment the following cell if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbzvFzUa-TOn","executionInfo":{"status":"ok","timestamp":1688928548256,"user_tz":-120,"elapsed":143581,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"94ee48f2-c27b-4cdc-e5a0-491161dde64a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.11.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.12.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.6.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (8.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.4)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2+cu118\n","    Uninstalling torchvision-0.15.2+cu118:\n","      Successfully uninstalled torchvision-0.15.2+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0+cu113 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0+cu113 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchvision-0.12.0+cu113\n","Collecting tensorboard==2.8.0\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.56.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.8.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.22.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.27.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (67.7.2)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.8.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.8.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (0.40.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.8.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (3.2.2)\n","Installing collected packages: tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tensorboard\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.3\n","    Uninstalling tensorboard-2.12.3:\n","      Successfully uninstalled tensorboard-2.12.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n","Collecting pytorch-lightning==1.6.0\n","  Downloading pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.1/582.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (1.22.4)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (1.11.0+cu113)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (4.65.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (6.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (2023.6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (2.8.0)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.6.0)\n","  Downloading torchmetrics-1.0.0-py3-none-any.whl (728 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning==1.6.0)\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (23.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.0) (4.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2.27.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (3.8.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.56.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.4.3)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (2.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.40.0)\n","Collecting lightning-utilities>=0.7.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.6.0)\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0) (3.2.2)\n","Installing collected packages: pyDeprecate, lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.9.0 pyDeprecate-0.3.2 pytorch-lightning-1.6.0 torchmetrics-1.0.0\n"]}],"source":["# Optional: install correct libraries in google colab\n","!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n","!python -m pip install tensorboard==2.8.0\n","!python -m pip install pytorch-lightning==1.6.0"]},{"cell_type":"markdown","metadata":{"id":"dlwn0i1I-TOn"},"source":["# 0. Setup\n","\n","As usual, we first import some packages to setup this notebook."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4IRRjHN6-TOo","executionInfo":{"status":"ok","timestamp":1688928549986,"user_tz":-120,"elapsed":1734,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}}},"outputs":[],"source":["import os\n","import torch\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","from exercise_code.rnn.sentiment_dataset import (\n","    create_dummy_data,\n","    download_data\n",")\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."]},{"cell_type":"markdown","metadata":{"id":"VizZmzGn-TOo"},"source":["# 1. Preprocessing a Text Classification Dataset\n","\n","As a starting point, let's load a dummy text classification dataset and have a sense of how it looks. We take these samples from the IMDb movie review dataset, which includes movie reviews and labels that show whether they are negative (0) or positive (1). You will investigate this task further in the second notebook.\n","\n","In this section, our goal is to create a text processing dataset. You are not required to write any code in this section. However, the concept introduced here is very important for working on NLP datasets in the future as well as in the rest of this exercise.\n","Take your time to understand the procedure here.\n","\n","First, let us download the data and take a look at some data samples."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hjv3_VUx-TOo","executionInfo":{"status":"ok","timestamp":1688928552237,"user_tz":-120,"elapsed":2254,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"7217a763-d9d0-4fa6-faa3-95cd0dbd1ac9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://i2dl.vc.in.tum.de/static/data/SentimentData.zip to /content/gdrive/MyDrive/i2dl/datasets/SentimentData/SentimentData.zip\n"]},{"output_type":"stream","name":"stderr","text":["3776512it [00:00, 14578413.04it/s]                           \n"]},{"output_type":"stream","name":"stdout","text":["Text: Adrian Pasdar is excellent is this film. He makes a fascinating woman.\n","Label: 1\n","\n","Text: This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments.\n","Label: 1\n","\n","Text: Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n","Label: 1\n","\n","Text: Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.\n","Label: 0\n","\n","Text: The characters are unlikeable and the script is awful. It's a waste of the talents of Deneuve and Auteuil.\n","Label: 0\n","\n","Text: I wouldn't rent this one even on dollar rental night.\n","Label: 0\n","\n"]}],"source":["i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"SentimentData\")\n","path = download_data(data_root)\n","data = create_dummy_data(path)\n","for text, label in data:\n","    print('Text: {}'.format(text))\n","    print('Label: {}'.format(label))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"lCW6Unxh-TOp"},"source":["## 1.1 Tokenizing Data\n","\n","As seen above, we loaded 3 positive and 3 negative reviews. Since the basic semantic unit of text is a word, the first thing we need to do is **tokenizing** the dataset, which means converting each review to a list of words."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wk5OwHzh-TOp","executionInfo":{"status":"ok","timestamp":1688928577576,"user_tz":-120,"elapsed":306,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"4873826f-2d96-45af-95f2-da8e39ce038f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(['adrian', 'pasdar', 'is', 'excellent', 'is', 'this', 'film', 'he', 'makes', 'a', 'fascinating', 'woman'], 1) \n","\n","(['this', 'is', 'the', 'definitive', 'movie', 'version', 'of', 'hamlet', 'branagh', 'cuts', 'nothing', 'but', 'there', 'are', 'no', 'wasted', 'moments'], 1) \n","\n","(['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville'], 1) \n","\n","(['great', 'movie', 'especially', 'the', 'music', 'etta', 'james', 'at', 'last', 'this', 'speaks', 'volumes', 'when', 'you', 'have', 'finally', 'found', 'that', 'special', 'someone'], 0) \n","\n","(['the', 'characters', 'are', 'unlikeable', 'and', 'the', 'script', 'is', 'awful', 'it', 's', 'a', 'waste', 'of', 'the', 'talents', 'of', 'deneuve', 'and', 'auteuil'], 0) \n","\n","(['i', 'wouldn', 't', 'rent', 'this', 'one', 'even', 'on', 'dollar', 'rental', 'night'], 0) \n","\n"]}],"source":["import re\n","\n","# use regular expression to split the sentence\n","# check https://docs.python.org/3/library/re.html for more information\n","def tokenize(text):\n","    return [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n","\n","tokenized_data = []\n","for text, label in data:\n","    tokenized_data.append((tokenize(text), label))\n","    print(tokenized_data[-1], '\\n')"]},{"cell_type":"markdown","metadata":{"id":"jDZi6sSD-TOp"},"source":["## 1.2 Creating a Vocabulary\n","\n","We have converted the dataset into pairs of token lists and corresponding labels. But strings have varying lengths, which is hard to handle. It would be nice to represent words with numbers. So, we need to create a <b>vocabulary</b>, which is a dictionary that maps each word to an integer id.\n","\n","In large datasets, there are too many words, and most of them don't occur very frequently. One common approach we use to tackle this problem is to pick the most common N words from the dataset. Therefore, we restrict the number of words.\n","\n","First, let's compute the word frequencies in our dummy dataset. To compute frequencies, we use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) data structure."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FblwDKTw-TOp","executionInfo":{"status":"ok","timestamp":1688928581938,"user_tz":-120,"elapsed":414,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"4cbb289d-c9da-4e7c-ee59-4cba8d5ca2f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'adrian': 1,\n","         'pasdar': 1,\n","         'is': 5,\n","         'excellent': 1,\n","         'this': 4,\n","         'film': 1,\n","         'he': 1,\n","         'makes': 1,\n","         'a': 2,\n","         'fascinating': 1,\n","         'woman': 1,\n","         'the': 6,\n","         'definitive': 1,\n","         'movie': 2,\n","         'version': 1,\n","         'of': 5,\n","         'hamlet': 1,\n","         'branagh': 1,\n","         'cuts': 1,\n","         'nothing': 1,\n","         'but': 1,\n","         'there': 1,\n","         'are': 2,\n","         'no': 1,\n","         'wasted': 1,\n","         'moments': 1,\n","         'smallville': 3,\n","         'episode': 3,\n","         'justice': 1,\n","         'best': 1,\n","         'it': 2,\n","         's': 2,\n","         'my': 1,\n","         'favorite': 1,\n","         'great': 1,\n","         'especially': 1,\n","         'music': 1,\n","         'etta': 1,\n","         'james': 1,\n","         'at': 1,\n","         'last': 1,\n","         'speaks': 1,\n","         'volumes': 1,\n","         'when': 1,\n","         'you': 1,\n","         'have': 1,\n","         'finally': 1,\n","         'found': 1,\n","         'that': 1,\n","         'special': 1,\n","         'someone': 1,\n","         'characters': 1,\n","         'unlikeable': 1,\n","         'and': 2,\n","         'script': 1,\n","         'awful': 1,\n","         'waste': 1,\n","         'talents': 1,\n","         'deneuve': 1,\n","         'auteuil': 1,\n","         'i': 1,\n","         'wouldn': 1,\n","         't': 1,\n","         'rent': 1,\n","         'one': 1,\n","         'even': 1,\n","         'on': 1,\n","         'dollar': 1,\n","         'rental': 1,\n","         'night': 1})"]},"metadata":{},"execution_count":6}],"source":["from collections import Counter\n","\n","freqs = Counter()\n","for tokens, _ in tokenized_data:\n","    freqs.update(tokens)\n","\n","freqs"]},{"cell_type":"markdown","metadata":{"id":"kXydF13i-TOp"},"source":["To create the dictionary, let's select the most common 20 words to create a vocabulary. In addition to the words that appear in our data, we need to have two special words:\n","\n","- `<eos>` End of sequence symbol used for padding\n","- `<unk>` Words unknown in our vocabulary"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fpPUReS-TOp","executionInfo":{"status":"ok","timestamp":1688928843268,"user_tz":-120,"elapsed":307,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"507fe615-1cfe-4fee-f3ac-bad8ccdbb907"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<eos>': 0,\n"," '<unk>': 1,\n"," 'the': 2,\n"," 'is': 3,\n"," 'of': 4,\n"," 'this': 5,\n"," 'smallville': 6,\n"," 'episode': 7,\n"," 'a': 8,\n"," 'movie': 9,\n"," 'are': 10,\n"," 'it': 11,\n"," 's': 12,\n"," 'and': 13,\n"," 'adrian': 14,\n"," 'pasdar': 15,\n"," 'excellent': 16,\n"," 'film': 17,\n"," 'he': 18,\n"," 'makes': 19,\n"," 'fascinating': 20,\n"," 'woman': 21}"]},"metadata":{},"execution_count":7}],"source":["vocab = {'<eos>': 0, '<unk>': 1}\n","for token, freq in freqs.most_common(20):\n","    vocab[token] = len(vocab)\n","vocab"]},{"cell_type":"markdown","metadata":{"id":"-luPNiy4-TOq"},"source":["## 1.3 Creating the Dataset\n","\n","Putting it all together, we can now create a dataset class. First, let's create index-label pairs:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhvHRHAa-TOq","executionInfo":{"status":"ok","timestamp":1688928867937,"user_tz":-120,"elapsed":608,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"49df9531-7736-459f-f14a-82035603f21c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[14, 15, 3, 16, 3, 5, 17, 18, 19, 8, 20, 21]  ->  1\n","\n","[5, 3, 2, 1, 9, 1, 4, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1]  ->  1\n","\n","[6, 7, 1, 3, 2, 1, 7, 4, 6, 11, 12, 1, 1, 7, 4, 6]  ->  1\n","\n","[1, 9, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  ->  0\n","\n","[2, 1, 10, 1, 13, 2, 1, 3, 1, 11, 12, 8, 1, 4, 2, 1, 4, 1, 13, 1]  ->  0\n","\n","[1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1]  ->  0\n","\n"]}],"source":["indexed_data = []\n","for tokens, label in tokenized_data:\n","    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n","    # the token that is not in vocab get assigned <unk>\n","    indexed_data.append((indices, label))\n","\n","\n","for indices, label in indexed_data:\n","    print(indices, ' -> ', label)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"s04hzWoG-TOq"},"source":["<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>We now use the PyTorch dataset class we provided in <code>exercise_code/rnn/sentiment_dataset.py</code> file. Please also take a look at the code.</p>\n"," </div>\n","    \n","\n","\n","Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ6chJWP-TOq","executionInfo":{"status":"ok","timestamp":1688928896840,"user_tz":-120,"elapsed":328,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"5e46813a-746c-4de6-db5c-aa867986ebda"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'data': tensor([1, 9, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(0.)}\n","\n","{'data': tensor([ 2,  1, 10,  1, 13,  2,  1,  3,  1, 11, 12,  8,  1,  4,  2,  1,  4,  1,\n","        13,  1]), 'label': tensor(0.)}\n","\n","{'data': tensor([ 5,  3,  2,  1,  9,  1,  4,  1,  1,  1,  1,  1,  1, 10,  1,  1,  1]), 'label': tensor(1.)}\n","\n","{'data': tensor([ 6,  7,  1,  3,  2,  1,  7,  4,  6, 11, 12,  1,  1,  7,  4,  6]), 'label': tensor(1.)}\n","\n","{'data': tensor([14, 15,  3, 16,  3,  5, 17, 18, 19,  8, 20, 21]), 'label': tensor(1.)}\n","\n","{'data': tensor([1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1]), 'label': tensor(0.)}\n","\n"]}],"source":["from exercise_code.rnn.sentiment_dataset import SentimentDataset\n","\n","combined_data = [\n","    (raw_text, tokens, indices, label)\n","    for (raw_text, label), (tokens, _), (indices, _)\n","    in zip(data, tokenized_data, indexed_data)\n","]\n","\n","dataset = SentimentDataset(combined_data)\n","\n","for elem in dataset:\n","    print(elem)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"IuRkT_QN-TOq"},"source":["## 1.4 Minibatching\n","Note that in the dataset we created, not all sequences have the same length. Therefore, __we cannot minibatch the data trivially__. This means we cannot use a `DataLoader` class easily.\n","\n","<b>If you uncomment the following cell and run it, you will very likely get an error!</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQj-a-wc-TOq"},"outputs":[],"source":["# loader = DataLoader(dataset, batch_size=3)\n","\n","# for batch in loader:\n","#     print(batch)"]},{"cell_type":"markdown","metadata":{"id":"jUsjeaz--TOq"},"source":["<div class=\"alert alert-success\">\n","    <h3>Task: Check Code</h3>\n","    <p>To solve the problem, we need to pad the sequences with <code> < eos > </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>exercise_code/rnn/sentiment_dataset</code>. </p>\n","    <p> In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0. </p>\n"," </div>"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5bFGxOKq-TOq","executionInfo":{"status":"ok","timestamp":1688929403799,"user_tz":-120,"elapsed":3,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"7cd0e8c0-e2c7-43b4-85c0-7ba5bb7f608f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: \n"," tensor([[ 1,  2,  5],\n","        [ 9,  1,  3],\n","        [ 1, 10,  2],\n","        [ 2,  1,  1],\n","        [ 1, 13,  9],\n","        [ 1,  2,  1],\n","        [ 1,  1,  4],\n","        [ 1,  3,  1],\n","        [ 1,  1,  1],\n","        [ 5, 11,  1],\n","        [ 1, 12,  1],\n","        [ 1,  8,  1],\n","        [ 1,  1,  1],\n","        [ 1,  4, 10],\n","        [ 1,  2,  1],\n","        [ 1,  1,  1],\n","        [ 1,  4,  1],\n","        [ 1,  1,  0],\n","        [ 1, 13,  0],\n","        [ 1,  1,  0]])\n","\n","Labels: \n"," tensor([0., 0., 1.])\n","\n","Sequence Lengths: \n"," tensor([20, 20, 17])\n","\n","\n","Data: \n"," tensor([[ 6, 14,  1],\n","        [ 7, 15,  1],\n","        [ 1,  3,  1],\n","        [ 3, 16,  1],\n","        [ 2,  3,  5],\n","        [ 1,  5,  1],\n","        [ 7, 17,  1],\n","        [ 4, 18,  1],\n","        [ 6, 19,  1],\n","        [11,  8,  1],\n","        [12, 20,  1],\n","        [ 1, 21,  0],\n","        [ 1,  0,  0],\n","        [ 7,  0,  0],\n","        [ 4,  0,  0],\n","        [ 6,  0,  0]])\n","\n","Labels: \n"," tensor([1., 1., 0.])\n","\n","Sequence Lengths: \n"," tensor([16, 12, 11])\n","\n","\n"]}],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def collate(batch):\n","    assert isinstance(batch, list)\n","    data = pad_sequence([b['data'] for b in batch])\n","    lengths = torch.tensor([len(b['data']) for b in batch])\n","    label = torch.stack([b['label'] for b in batch])\n","    return {\n","        'data': data,\n","        'label': label,\n","        'lengths': lengths\n","    }\n","\n","loader = DataLoader(dataset, batch_size=3, collate_fn=collate)\n","for batch in loader:\n","    print('Data: \\n', batch['data'])\n","    print('\\nLabels: \\n', batch['label'])\n","    print('\\nSequence Lengths: \\n', batch['lengths'])\n","    print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"9_dVYfL7-TOq"},"source":["We can see that these two batches have different length, this is how the reverse sort mentioned in `1.3 Creating the Dataset` benefits for less memory and less computation."]},{"cell_type":"markdown","metadata":{"id":"i0m7dDwc-TOr"},"source":["# 2. Embeddings\n","\n","In the previous section, we explored how to convert text into a sequence of integers. In this form, sequences are still not ready to be inputs of RNNs you implemented in the optional notebook.\n","\n","An integer representation is usually a one-hot encoding, while not the same since they are not equally weighted given only an integer.\n","\n","Moreover, it fails to express the semantic relations between words and the order of the words has no meaning. We would like a better representation to keep the semantic meaning of the word. For example, as shown in the following picture, the difference between man and woman and the difference between king and queen should be close, since the difference is only the gender. If we use a vector for each word, the above relation can be expressed as $vec(\\text{women})-vec(\\text{man}) \\approx vec(\\text{queen}) - vec(\\text{king})$. Usually we call such vector representations as embeddings.\n","\n","<img src='https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg' width=80% height=80%/>\n","\n","While one can use pre-trained embedding vectors such as [word2vec](https://arxiv.org/abs/1301.3781) or [GLoVe](https://nlp.stanford.edu/projects/glove/), in this exercise we use randomly initialized embedding vectors that will be trained from scratch together with our networks. As we train our model, it will learn the semantic relations between words."]},{"cell_type":"markdown","metadata":{"id":"OUtS9EFh-TOr"},"source":["<div class=\"alert alert-info\">\n","\n","<h3> Task: Implement Embedding</h3>\n"," <p>In this part, you will implement a simple embedding layer. Embedding is a simple lookup table that stores a dense vector to represent each word in the vocabulary.</p>\n","\n"," <p>Your task is to implement the <code>Embedding</code> class in <code>exercise_code.rnn.rnn_nn</code> file. Once you are done, run the below cell to test your implementation. Note that we ensure eos embeddings to be zero by using the <code>padding_idx</code> argument.\n","\n"," </div>"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYWAH6Uw-TOr","executionInfo":{"status":"ok","timestamp":1688930499968,"user_tz":-120,"elapsed":886,"user":{"displayName":"Arij Ben rhouma","userId":"17550122761114616191"}},"outputId":"8251a04e-0fde-4c60-ebd9-6fc9d659cc49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between outputs: 0.0\n","Test passed :)!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}],"source":["import torch.nn as nn\n","\n","from exercise_code.rnn.rnn_nn import Embedding\n","from exercise_code.rnn.tests import embedding_output_test\n","\n","\n","i2dl_embedding = Embedding(len(vocab), 16, padding_idx=0)\n","pytorch_embedding = nn.Embedding(len(vocab), 16, padding_idx=0)\n","\n","loader = DataLoader(dataset, batch_size=len(dataset), collate_fn=collate)\n","for batch in loader:\n","    x = batch['data']\n","\n","embedding_output_test(i2dl_embedding, pytorch_embedding, x)\n"]},{"cell_type":"markdown","metadata":{"id":"RUjMgi_L-TOr"},"source":["# 3. Conclusion\n","\n","In this notebook, you learned how to prepare text data and how to create an embedding layer. In the next notebook, you will combine your Embedding and RNN implementations to create a sentiment analysis network!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"ae3aae73068e3f6c78354faadc00aa3f23e0713f86a27300232dd83e2bc002d8"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}